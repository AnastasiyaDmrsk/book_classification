{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install joblib\n",
    "!pip install matplotlib"
   ],
   "id": "c858c6a6c7f4c91a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Step 1: Drop rows with missing descriptions or categories\n",
    "df_clean = df.dropna(subset=[\"description\", \"categories\"]).copy()\n",
    "\n",
    "# Step 2: Keep only the first genre if multiple are present\n",
    "df_clean[\"main_category\"] = df_clean[\"categories\"].apply(lambda x: x.split(\";\")[0].strip())\n",
    "\n",
    "# Step 3: Keep only the top 10 most frequent genres\n",
    "top_categories = df_clean[\"main_category\"].value_counts().nlargest(10).index.tolist()\n",
    "df_top10 = df_clean[df_clean[\"main_category\"].isin(top_categories)]\n",
    "\n",
    "# Step 4: Prepare features (X) and labels (y)\n",
    "X = df_top10[\"description\"]\n",
    "y = df_top10[\"main_category\"]\n",
    "\n",
    "# Encode string labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 5: Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 6: Vectorize descriptions using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# === Train Models ===\n",
    "\n",
    "# Naive Bayes\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_vec, y_train)\n",
    "y_pred_nb = model_nb.predict(X_test_vec)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_vec)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_vec)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_vec, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_vec)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "\n",
    "# === Detailed Evaluation for Naive Bayes ===\n",
    "present_labels = unique_labels(y_test, y_pred_nb)\n",
    "present_class_names = label_encoder.inverse_transform(present_labels)\n",
    "\n",
    "report = classification_report(\n",
    "    y_test, y_pred_nb,\n",
    "    labels=present_labels,\n",
    "    target_names=present_class_names,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "top_f1 = report_df.sort_values(by=\"f1-score\", ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 genres by F1-score (Naive Bayes):\")\n",
    "print(top_f1)\n",
    "\n",
    "# === Confusion Matrix for Naive Bayes ===\n",
    "cm = confusion_matrix(y_test, y_pred_nb, labels=present_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=present_class_names,\n",
    "            yticklabels=present_class_names)\n",
    "plt.xlabel(\"Predicted Genre\")\n",
    "plt.ylabel(\"True Genre\")\n",
    "plt.title(\"Confusion Matrix (Top 10 Genres) - Naive Bayes\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Compare F1-score (macro) for all models ===\n",
    "f1_scores = {\n",
    "    \"Naive Bayes\": f1_score(y_test, y_pred_nb, average=\"macro\", zero_division=0),\n",
    "    \"Logistic Regression\": f1_score(y_test, y_pred_lr, average=\"macro\", zero_division=0),\n",
    "    \"Random Forest\": f1_score(y_test, y_pred_rf, average=\"macro\", zero_division=0),\n",
    "    \"SVM\": f1_score(y_test, y_pred_svm, average=\"macro\", zero_division=0)\n",
    "}\n",
    "# Print F1-scores\n",
    "print(\"\\nüîç F1-score (macro) for all models:\")\n",
    "for model_name, score in f1_scores.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_f1_score = f1_scores[best_model_name]\n",
    "print(f\"\\nüèÜ Best model based on macro F1-score: {best_model_name} ({best_f1_score:.4f})\")\n",
    "\n",
    "# Plot F1-score comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette=\"viridis\")\n",
    "plt.title(\"Model Comparison by F1-score (macro)\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Save all models and encoders ===\n",
    "joblib.dump(model_nb, \"genre_classifier_nb.pkl\")\n",
    "joblib.dump(log_reg, \"genre_classifier_logreg.pkl\")\n",
    "joblib.dump(rf_model, \"genre_classifier_rf.pkl\")\n",
    "joblib.dump(svm_model, \"genre_classifier_svm.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ All models and preprocessing objects saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
